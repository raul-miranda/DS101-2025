---
title: "PROJECT 3 - High Tech Potential of Countries"
author: "Raul Miranda"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Do the necessary library installations

```{r}
# install.packages("tidyverse","pROC")
    library(tidyverse)
    library(dplyr)
    library(ggplot2)
    library(pROC)
    library(caret)


```

# Title: High-Tech Potential of Countries
<br>

## a. Introduction

The primary question in this study is: **what makes countries the most productive in high tech industries?** 
In essence, we would like to identify the countries that are in the upper quantile of countries that achieved high productivity in high-tech in the 2000-2024 period. Those that did are labeled High and the rest are labeled Low. Since status may change over time, we will find some countries oscillating between High and Low. 

In order to answer the question, let's define productivity in terms of measurable indicators found in the databases provided. After some considerations, I arrived at the set of indicators shown below, and that are accessible in World Bank Indicators. They include (1) direct measures of national productivity: High-technology exports, ICT goods exports, ICT service exports and Scientific and technical journal articles; (2) measures of national resources: Access to electricity, Internet users, and Secure internet servers; and (3) measures of research promotion:  Research and development expenditure, and Researchers in R&D. 

World Bank lacked a single indicator that quantitatively evaluates overall performance, specifically in high-tech. "High-tech" includes advanced technologies derived from scientific achievements in medicine, industry, information, space, etc. I found a couple of good indicators, but was particularly impressed by the United Nations Frontier Technology Readiness Index (see reference below). A body of experts have tracked and maintained this performance index since 2010 until 2023 most recently and publishes databases and perspectives. The **Overall performance index** for countries is a number between 0 and 1.  Its statistical distribution shows the 3 quantiles, with the upper (3Q) in the range of 0.70 and 1.

I used that FTRI index to label a categorical variable I called tech_readiness (Technological Readiness), which acquires two values: Low for 0.0 $\le$ FTRI < 0.7, and High for 0.7 $\le$ FTRI $\le$ 1.0. 

In this project I use Logistic Regression to classify countries as Low or High for a given year over the 2000-2024 period. The predictor variables are the World Bank indicators, and the outcome variable is the Technological Readiness label derived from the FTRI index.   Since the FTRI index is lacking before 2010, I developed a model based on the 2010 to 2023 dataset, tested its statistical accuracy, and used it to predict the 2000-2024 Technological Readines of countries, based on the reported technological indices given by the World Bank.

I focused mainly on the logistic regression modeling task, and didn't have time to go into the most useful details of studying which economic indicators characterize the higher performers.
<br>

## b. Source databases. Variable definition

Downloaded to my project folder a compiled set of World Bank Indicators from http://data.worldbank.org/, including the indicators mentioned below. Name of the dataset in my folder:  worlbankdataset.csv.

In addition, from the UN Frontier Technology Readiness Index (https://unctadstat.unctad.org/datacentre/) downloaded 
https://unctadstat.unctad.org/datacentre/dataviewer/US.FTRI. Name of the dataset in my folder: FTRI_table.csv. 

The original table had the M-49 country codes (used by the UN) but lacked the ISO-3 country codes (used by the World Bank), so I used a conversion table from https://unstats.un.org/unsd/methodology/m49/overview/. 

Datasets are tabulated by country_name, country_code and year. The original bound data set contains about 5400 records in the 2000-2024 period (long format), but not all countries reported complete information.

Outcome variable (categorical): tech_readiness (high, low)

From https://unctadstat.unctad.org/datacentre/reportInfo/US.FTRI -- I defined the indicator Technological Readiness based on stats of FTRI Overall Index: 1Q-3Q: 0-0.3-0.5-0.7-1.0.  High tech_readiness corresponds to FTRI Overall Index $\ge$ 0.70.

From http://data.worldbank.org/, predictor variables:

Access to electricity (% of population) - quantitative

Internet users (% of population) - quantitative

Research and development expenditure (% of GDP) - quantitative

Researchers in R&D (per million people) - quantitative

Scientific and technical journal articles - quantitative

High-technology exports (% of manufactured exports) - quantitative

Secure Internet servers - quantitative

ICT goods exports (% of total goods exports) - quantitative

ICT service exports (% of service exports, BoP) - quantitative
<br>

## c. Dataset preparation and cleaning

Read datasets and bind them.  Examine the top rows to take a peek at its variable structure.


```{r}
setwd(getwd()) # ensure we're at the directory where the RMD is running
worldbank <- read_csv("worldbankdataset.csv", show_col_types = FALSE)
ftri <- read_csv("FTRI_table.csv", show_col_types = FALSE)

combined_set <- left_join(worldbank, ftri, by= c("country_code" = "code", "year"="year"))

head (combined_set, 3)
str(combined_set)
```
<br>

Get the basic stats of the FTRI overall_index

```{r}

summary(combined_set$overall_index)

```
<br>

Define the outcome variable tech_readiness  (high, low) based on overall_index $\ge$ 0.7 for "high". Then make it a factor for glm to work, and relevel the ref to "low".

```{r}
combined_set <- combined_set|> mutate (tech_readiness= if_else(overall_index >= 0.7, "high", "low"))
combined_set$tech_readiness <- as.factor(combined_set$tech_readiness)
combined_set$tech_readiness <- relevel(combined_set$tech_readiness, ref= "low")
summary (combined_set)

```
<br>

Since the FTRI data is complete between 2010 and 2023, except for Syria and island states, we will reduce the dataset, to exclude years outside 2010-23 and country/years that don't have tech_readiness score: Syria and small island states. After fitting a logistic regression model and testing its accuracy, we will impute the missing tech_readiness with values predicted by the regression model for the 2000-2009 period and 2024.


```{r}
combined_reduced <- combined_set |>
      filter (year >= 2010 & year <= 2023 & !is.na(tech_readiness))     # the reduced set contains ca 2400 rows
#view(combined_reduced)   #remove comment to view set
colSums(is.na(combined_reduced))
              
              
```
<br>

## d. Exploratory data analysis

We'll look at the boxplots of the variables grouped by tech_readiness.

```{r}

vars <- c("scientific_articles","access_electricity","high_tech_exports","ict_service_exports","ict_goods_exports","researchers_millpop","secure_servers","internet_users","rd_expenditures_gdp")

for (var in vars) {
  boxplot(combined_reduced[[var]] ~ combined_reduced$tech_readiness,
          main = paste(var),
          ylab = var,
          col = "lightblue",
          border = "darkblue"
          )
}

```
<br>
**Conclusions**

For most variables, there is clear distinction of means/medians between the two groups (low and high). Some of the variables appear normally distributed within each group, with a few outliers. Others show many significantly separated outliers. Those outliers actually fall above the 70% of the distribution for each variable. So, they play an important role in separating the highest performing countries/years from the rest. 

Variable rd_expenditures seems to be highly influential in separating the groups. Also important appear to be scientific_articles, high_tech_exports, and researchers_millpop.  secure_servers appear to completely separate low from high, so it may cause instability in the regression. 

To appropriately analyze this dataset that presents variable distributions not perfectly normal, an appropriate logistic method should be sought or at least compared with the outcome from glm() which assumes normal distribution.

However, in order to carry out this project within a reasonable time I will only use the glm() method learned in class. We will actually see whether glm() can produce a reliable and accurate model to explain this dataset.


<br>

## e. Statistical analysis by logistic regression


After studying logistic regression, I learned that the best approach to develop a model is to train the model with a subset of the dataset (training set) and to test its accuracy with the remaining rows (the test set).

I will follow an approach described by Wickham and will use createDataPartition (from library caret), to split the data into a training set (80%) and a test set (20%). Base R has sample() but that function doesn't balance the proportion of low/high in tech_readiness. createDataPartition preserves the proportion of each class (high, low) in both the training and test sets.


```{r}

set.seed(123) #   set.seed generates a reproducible seed for the  random number generator used by createDataPartition.

trainset_idx <- createDataPartition(y = combined_reduced$tech_readiness, p = 0.8, list = FALSE)

combined_red_train <- combined_reduced[trainset_idx, ]    # this is the training set, with ca 1930 rows
combined_red_test <- combined_reduced[-trainset_idx, ]    # this is the test set, with ca 480 rows

```

<br>

**Run with the training dataset**

Now go for logistic regression with the training dataset with tech_readiness as output and 9 vars as predictors

```{r}

model_train <- glm(tech_readiness ~ scientific_articles + access_electricity + 
                     high_tech_exports + ict_service_exports +  ict_goods_exports +  
                 researchers_millpop +  secure_servers + internet_users + rd_expenditures_gdp, 
                        family = binomial, data = combined_red_train)
summary(model_train)




```
<br>
**Interpretation of the logistic regression and its accuracy**

The deviances are measures of goodness of fit. Null d. is a measure of residuals with a model that is equal to the intercept term. Residual d. is a measure of residuals considering the intercept and all the variables. A large difference Null - Residual (2170-634) indicates a good fit as the model predicts the tech_readiness outcome with smaller deviance than just a null (intercept-only) model.  The parenthesis about the dispersion parameter taken as 1 means that the model assumes normal distribution of the data and variance.

The regression coefficients show the change in the log odds of the tech_readiness for a unit change of each predictor variable. So, for example, a 1% increase of %people with access to electricity, increases the log odds of tech_readiness by roughly 0.5. While a 1% increase of %people that are internet_users, increases the log odds of tech_readiness by 0.01, substantially less than electricity_access. We can also see that a 1% of GDP increase in rd_expenditures increases the log odds of tech_readiness by roughly 1, a very significant effect.

The p-value of the intercept and 6 of the variables is well below 0.05, meaning that the calculated coefficients are statistically significant within 95% confidence. Four of them show p-value of almost 0, high_tech_exports, researchers_millpop, secure_servers, and rd_expenditures. While three of them show p-values above 0.05, ict_service_exports, ict_goods_exports and internet_users, meaning that their coefficients are not statistically significance with 95% confidence.

To calculate the p-value of the logistic model compared to the null model we can do :

```{r}
with(model_train, null.deviance - deviance)

with(model_train, df.null - df.residual)


with(model_train, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

<br>

This again shows the difference in deviance between a null and a full model. 9 are the degrees of freedom.  And the 
p-value is 0, showing that the model explains over 95% of the variance (deviances).


Now we will obtain the odds ratios by exponentiating the intercept and coefficients, and will get the confidence intervals (confint), and will show those side by side. 

```{r}

exp(coef(model_train))
exp(cbind(OR = coef(model_train), confint(model_train)))
```
<br>

The Warnings show that in a number of cases the prediction is exactly 0 or 1, which is a sign of complete separation or quasi-complete separation, ie one or more predictor variables correlating exactly with the tech_readiness outcome.  We already saw this potential problem with secure_servers in the EDA section. However, the p-values lend confidence in the value of coefficients for most (except 3) variables.  And moreover, the model converged so I will continue to explore it.

Reading about this problem, it seems that a possible approach is to reduce the number of variables, or to scale and center the variables. I tried scaling the variables but the regression results were identical to those derived with the unscaled variables.  If I have time, I will later try reducing by eliminating the variables with the highest p-value coefficients, namely ict_service exports and ict_goods_exports, since they overlap to some degree with high_tech_exports, and secure_servers.

Before doing that, though, let's get to calculate the confusion table and the ROC curve with the full 9-variable model.

Using the model (model_train) predict probabilities and the the predicted technological readiness (pred_tr) by using a cutoff of the sigmoid curve of 0.4, above which is labeled "high" and below which is labeled "low".


```{r}
pred_prob <- predict(model_train, type = "response")
pred_tr <- ifelse(pred_prob >= 0.4, "high", "low") 

compare_thetwo <- data.frame(ActualTR = combined_red_train$tech_readiness, PredictedTR = pred_tr)

```

<br>

Create the confusion table of tech_readiness, with Actual values in the rows, and Predicted values in the columns.

```{r}
summ_compare <- table(compare_thetwo$ActualTR, compare_thetwo$PredictedTR)
summ_compare



```

<br>

Now do the accuracy eval; with 'positive'='high', 'negative'='low'.


```{r}
TP <- 423; TN <- 1397; FP <- 57; FN <- 58

Accuracy <- (TP + TN) / (TP + TN + FP + FN)
Precision <- TP / (TP + FP)
Sensitivity <- TP / (TP + FN)  # Recall
Specificity <- TN / (TN + FP)

F1_Score <- 2 * (Precision * Sensitivity) / (Precision + Sensitivity)
cat (" Accuracy: ",Accuracy,"\n","Precision: ",Precision,"\n", "Recall/sensitivity: ", Sensitivity, "\n", "Specificity: ", Specificity, "\n","F1_Score: ", F1_Score)
```
<br>

These numbers confirm the reliability of the full model. The F1_score close to 1 indicates good balance of precision and recall.


<br>

DO the ROC curve and AUC:


```{r}
library(pROC)

roc_obj <- roc(response = combined_red_train$tech_readiness,
                predictor = pred_prob,
                direction = "<")  # smaller prob = low


# Print AUC value
auc_val <- auc(roc_obj); auc_val


```
<br>

With an AUC close to 1 (and far from 0.5) the model shows high accuracy in classifying highs and lows.

<br>

Now plot the ROC with AUC displayed

```{r}
plot.roc(roc_obj, print.auc = TRUE, legacy.axes = TRUE,
         xlab = "False Positive Rate (1 - Specificity)",
         ylab = "True Positive Rate (Sensitivity)")

```


<br>

**Conclusion**

Model model_train is accurate and precise and explains the deviances of the training set well.  To further assess the accuracy of this model, now use the test data subset.

<br>

**Run with the Testing Set**

Now further test the accuracy of model_train using the test dataset. Predict probs from the test set we called combined_red_test, and then set the threshold in the sigmoid as before, to 0.4.  ("high": $\ge$ 0.4)


```{r}
pred_prob <- predict(model_train, newdata= combined_red_test, type = "response")
pred_tr <- ifelse(pred_prob >= 0.4, "high", "low") 

compare_thetwo <- data.frame(ActualTR = combined_red_test$tech_readiness, PredictedTR = pred_tr)

```


<br>
Create the confusion matrix for tech_readiness from the predictions obtained with test set.
```{r}
summ_compare <- table(compare_thetwo$ActualTR, compare_thetwo$PredictedTR)
summ_compare



```
<br>

Now do the accuracy eval; with 'positive'='high', 'negative'='low'.


```{r}
TP <- 102; TN <- 353; FP <- 10; FN <- 18

Accuracy <- (TP + TN) / (TP + TN + FP + FN)
Precision <- TP / (TP + FP)
Sensitivity <- TP / (TP + FN)  # Recall
Specificity <- TN / (TN + FP)

F1_Score <- 2 * (Precision * Sensitivity) / (Precision + Sensitivity)
cat (" Accuracy: ",Accuracy,"\n","Precision: ",Precision,"\n", "Recall/sensitivity: ", Sensitivity, "\n", "Specificity: ", Specificity, "\n","F1_Score: ", F1_Score)
```
<br>

These numbers again confirm the reliability of the full model. The F1_score close to 1 indicates good balance of precision and recall.


<br>


Now do the ROC curve and AUC.


```{r}
library(pROC)

roc_obj <- roc(response = combined_red_test$tech_readiness,
                predictor = pred_prob,
               direction = "<")  # smaller prob = low


# Print AUC value
auc_val <- auc(roc_obj); auc_val


```
<br>

With an AUC close to 1 (and far from 0.5) the model shows high accuracy in classifying highs and lows also in the test set.
<br>
Plot ROC with AUC displayed

```{r}
plot.roc(roc_obj, print.auc = TRUE, legacy.axes = TRUE,
         xlab = "False Positive Rate (1 - Specificity)",
         ylab = "True Positive Rate (Sensitivity)")

```
<br>
**Conclusion**

Model model_train displays high accuracy, specificity and sensitivity, with the test dataset,  equally as well as with the training set.


<br>
**Backfill the missing Technical Readiness values**

Since model_train accuracy is equally high, above 94% for both test and train sets,  let's go ahead and use the model_train to impute now all of the missing technical readiness levels in the FTR index data table.

We will use the original World Bank data for the other columns to predict (and back fill) the Tech_readiness since 2000, but only for the missing values of tech_readiness in the combined_set. We will not override the existing true values with the predicted ones.

In addition, I noticed that many countries including many of the developed ones, reported scarce statistics for 2024, so, rather than dropping 2024, I will also impute the 2024 variable values reported as zero, with the 2023 values, since most countries will not change their technological readiness status in a year. Notice that if 2023 value is zero, 2024 will also be zero, so 2024 will not be inflated.

Impute the 2024 values of the variables in var_list (when they are zero) with the 2023 values:

```{r}

var_list <- c("scientific_articles", "access_electricity", "high_tech_exports", "ict_service_exports", "ict_goods_exports", "researchers_millpop", "secure_servers", "internet_users", "rd_expenditures_gdp")

impute_var <- function(var,year,targety) {
  case_when (
    year == targety & var == 0 ~ lag(var, n=1), TRUE ~var) }      # use  lag() to get value of previous year
                                                                # leave var untouched if not zero

combined_set <- combined_set |> group_by(country_code) |>
mutate(across(all_of(var_list), ~impute_var(.x,year,2023)))  |>    # impute 2023 when it is zero
mutate(across(all_of(var_list), ~impute_var(.x,year,2024)))  |>    # impute 2024 when it is zero
ungroup()
```

<br>
Now predict the probabilities for the full combined_set (5400 rows) using the successful model_train:

```{r}
combined_set$predprob <- predict(model_train, newdata= combined_set, type= "response")

# And then determine pred_tech_readiness this way: When tech_readiness is available in combined_set, pred_tech_readiness = tech_readiness (the true value).  When tech_readiness is NA, pred_tech_readiness = high or low (depending on whether predprob (probability) is above or below threshold of 0.4). 

combined_set$pred_tech_readiness <- if_else(is.na(combined_set$tech_readiness), if_else(combined_set$predprob >=.4, "high","low"), combined_set$tech_readiness)


```

<br>


Check how the basic statistics have evolved between "original" set (2010-2023) and "predicted" set (2000-2024). The ratio High/Low is higher for the "original" 2010-2023 period than for the "predicted" 2000-2024 because some countries became more productive and entered the "high" status in the latter decade compared to the early 2000s.

```{r}

combined_set |>
  summarize( Orig_high = sum(tech_readiness == "high", na.rm = TRUE), Orig_low = sum(tech_readiness == "low", na.rm = TRUE),
             OrigHiLowratio = Orig_high/Orig_low,
             Pred_high = sum(pred_tech_readiness == "high", na.rm = TRUE), Pred_low = sum(pred_tech_readiness == "low", na.rm = TRUE),
             PredHiLowratio = Pred_high/Pred_low ) |> print()

```

<br>

**Time-evolution of countries and indicators**

Show the evolution over time: in the aggregate depict the yearly progress of country evolving from Low to High, and of a couple of crucial independent indicators: R&D Expenditures and High-Tech Exports.

```{r}
combined_set_agg <- combined_set |>
  group_by(year, pred_tech_readiness) |>
  summarize(`R&D Expenditures` = mean(rd_expenditures_gdp), `High-Tech Exports` = mean(high_tech_exports), Countries = n(), .groups = 'drop')


ggplot(combined_set_agg, aes(x = year)) + 
  geom_point(aes(y = Countries, color = pred_tech_readiness , shape = pred_tech_readiness ), size = 3) +
  labs(
    title = "World Evolution towards High-Tech in 2000-2024",
    x = "Year", y = "Countries",
    color = "Technological Readiness",
    shape = "Technological Readiness"
  ) +
  theme_minimal() 

ggplot(combined_set_agg, aes(x = year)) + 
  geom_point(aes(y = `High-Tech Exports`, color = pred_tech_readiness , shape = pred_tech_readiness ), size = 3) +
  labs(
    title = "World Evolution towards High-Tech in 2000-2024",
    x = "Year", y = "High-Tech Exports (% all exports)",
    color = "Technological Readiness",
    shape = "Technological Readiness"
  ) +
  theme_minimal() 

ggplot(combined_set_agg, aes(x = year)) +
  geom_point(aes(y = `R&D Expenditures`, color = pred_tech_readiness , shape = pred_tech_readiness ), size = 3) +
  labs(
    title = "World Evolution towards High-Tech in 2000-2024",
    x = "Year", y = "R&D Expenditures (mean %GDP)",
    color = "Technological Readiness",
    shape = "Technological Readiness"
  ) +
  theme_minimal() 
```
<br>

The evolution over the 2000-2025 period shows that countries/year-performance have progressively migrated from "low" to "high" in the average.  The curves for R&D expenditure and High-Tech exports clearly show the distinct behavior of "high" versus "low" performers, and point to two key variables that characterize the two classes. Policy makers can make use of this model to assess the effect of increasing R&D investment on enhancing the country's technological readiness. 


<br>

## f. Overall Conclusions

A logistic regression model to classify countries into high and low in terms of technological readiness was developed on the basis of economic indicators available in World Bank, and the FTRI technology readiness index available at the United Nations databases. The model was trained with a subset of the data and further tested for accuracy with another subset.

The logistic regression model is accurate and is able to predict technical readiness on the basis of the variables selected. The classification into high and low technical readiness allows an analysis of which factors are most relevant to promote countries to evolve from low to high.  Two very important factors are R&D expenditures and high-technology exports. Surprisingly, the factors that are least significant in the model are ICT (information and communication technology) goods and services, despite their relevance to high-tech. Other factors have medium-level significance: number of researchers, scientific_articles, access to electricity, number of secure_servers, and percent of internet_users in the population.

The time evolution of the countries from one class to the other (low to high) shows that it is very difficult to "close the gap". Two of the indicators display that clearly. The R&D Expenditure gap is about 1.5% GDP and it has decreased over the past decade because the "high" countries are spending less. In parallel, the High-Tech Exports gap is 15% of manufactured exports. The gap is widening in the last decade as "high countries" export more and "low countries" export less.

An important outcome from this project was to be able to predict Technological Readiness for 2000-2024, beyond the range covered by the FTRI index from 2010-2023. 


## g. Obstacles

Assembling the database from two different sources was challenging, as the process generated NAs for years not covered by one of the other source.  The modeling itself demanded studying some case studies of applications of logistical regression, as interpretation is more difficult than with linear regression. We learned about the occurrence of complete separation, which needs to be dealt with in various ways. The regression generated a few warnings of complete separation, but at the end we showed that the model was highly accurate and didn't need special treatment for the model to converge. 

## h. Future steps

The glm() model appears to be accurate and reliable after examining the outcome and statistical significance of coefficients and the model. However, the warnings of complete or quasi complete separation indicates that further work is needed to ensure the reliability. Variable scaling and centering, as well as reducing the number of variables can be tried to see whether the warnings disappear. Reading about this, it seems that there are other logistic regression methods that are better suited for non-normally distributed data. Hence learning about those methods could be tried as well. 

## i. References

. Class notes

. H. Wickham, et al., R for Data Science, 2nd Ed., https://r4ds.hadley.nz/ (2023)

. United Nations, Frontier Technology Readiness Index, annual. Understanding the index and definitions of each index. https://unctadstat.unctad.org/datacentre/reportInfo/US.FTRI, accessed in November 2025.


## j. Published

Published at https://rpubs.com/rmiranda/1373253


